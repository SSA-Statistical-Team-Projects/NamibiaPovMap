xlab("Index") +
ylab("Cook's Distance") +
theme_minimal()
# Plot with outliers highlighted
cooks_dt %>%
ggplot(aes(x = index, y = cooksdist)) +
geom_segment(aes(x = index, y = 0, xend = index, yend = cooksdist, color = is_outlier)) +
scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) + # Color outliers red
xlab("Index") +
ylab("Cook's Distance") +
ggtitle("Cook's Distance Plot with Stricter Outlier Detection") +
theme_minimal() +
# Label the outlier points
geom_text(
data = filter(cooks_dt, is_outlier),
aes(label = index),
vjust = -1,
color = "red"
)
threshold <- 100 * mean(cooks_dt$cooksdist, na.rm = TRUE)      # Common rule of thumb
# Plot with outliers highlighted
cooks_dt %>%
ggplot(aes(x = index, y = cooksdist)) +
geom_segment(aes(x = index, y = 0, xend = index, yend = cooksdist, color = is_outlier)) +
scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) + # Color outliers red
xlab("Index") +
ylab("Cook's Distance") +
ggtitle("Cook's Distance Plot with Stricter Outlier Detection") +
theme_minimal() +
# Label the outlier points
geom_text(
data = filter(cooks_dt, is_outlier),
aes(label = index),
vjust = -1,
color = "red"
)
# Assuming `log_model$model` is your model, and you are computing Cook's distance
cooksdist <- cooks.distance(log_model$model)
# Create a data table with Cook's Distance and indices
cooks_dt <- data.table(index = 1:length(cooksdist), cooksdist = cooksdist)
# Determine the threshold for outliers
n <- length(cooksdist)  # Number of observations
threshold <- 60 * mean(cooks_dt$cooksdist, na.rm = TRUE)      # Common rule of thumb
# Add a column to flag outliers
cooks_dt <- cooks_dt %>%
mutate(is_outlier = cooksdist > threshold)
# Plot with outliers highlighted
cooks_dt %>%
ggplot(aes(x = index, y = cooksdist)) +
geom_segment(aes(x = index, y = 0, xend = index, yend = cooksdist, color = is_outlier)) +
scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) + # Color outliers red
xlab("Index") +
ylab("Cook's Distance") +
theme_minimal() +
# Label the outlier points
geom_text(
data = filter(cooks_dt, is_outlier),
aes(label = index),
vjust = -1,
color = "red"
)
threshold <- 50 * mean(cooks_dt$cooksdist, na.rm = TRUE)      # Common rule of thumb
# Add a column to flag outliers
cooks_dt <- cooks_dt %>%
mutate(is_outlier = cooksdist > threshold)
# Plot with outliers highlighted
cooks_dt %>%
ggplot(aes(x = index, y = cooksdist)) +
geom_segment(aes(x = index, y = 0, xend = index, yend = cooksdist, color = is_outlier)) +
scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) + # Color outliers red
xlab("Index") +
ylab("Cook's Distance") +
theme_minimal() +
# Label the outlier points
geom_text(
data = filter(cooks_dt, is_outlier),
aes(label = index),
vjust = -1,
color = "red"
)
# Assuming `log_model$model` is your model, and you are computing Cook's distance
cooksdist <- cooks.distance(log_model$model)
# Create a data table with Cook's Distance and indices
cooks_dt <- data.table(index = 1:length(cooksdist), cooksdist = cooksdist)
# Determine the threshold for outliers
n <- length(cooksdist)  # Number of observations
threshold <- 50 * mean(cooks_dt$cooksdist, na.rm = TRUE)      # Common rule of thumb
# Add a column to flag outliers
cooks_dt <- cooks_dt %>%
mutate(is_outlier = cooksdist > threshold)
# Plot with outliers highlighted
cooks_dt %>%
ggplot(aes(x = index, y = cooksdist)) +
geom_segment(aes(x = index, y = 0, xend = index, yend = cooksdist, color = is_outlier)) +
scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) + # Color outliers red
xlab("Index") +
ylab("Cook's Distance") +
theme_minimal() +
# Label the outlier points
geom_text(
data = filter(cooks_dt, is_outlier),
aes(label = index),
vjust = -1,
color = "red"
)
# Plot with outliers highlighted
cooks_dt %>%
ggplot(aes(x = index, y = cooksdist)) +
geom_segment(aes(x = index, y = 0, xend = index, yend = cooksdist, color = is_outlier)) +
scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red"),
title = "Outlier") + # Color outliers red
xlab("Index") +
ylab("Cook's Distance") +
theme_minimal() +
# Label the outlier points
geom_text(
data = filter(cooks_dt, is_outlier),
aes(label = index),
vjust = -1,
color = "red"
)
cooks_dt %>%
ggplot(aes(x = index, y = cooksdist)) +
geom_segment(aes(x = index, y = 0, xend = index, yend = cooksdist, color = is_outlier)) +
scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red"),
labels = "Outlier") + # Color outliers red
xlab("Index") +
ylab("Cook's Distance") +
theme_minimal() +
# Label the outlier points
geom_text(
data = filter(cooks_dt, is_outlier),
aes(label = index),
vjust = -1,
color = "red"
)
# Add a column to flag outliers
cooks_dt <- cooks_dt %>%
mutate(Outlier = cooksdist > threshold)
# Plot with outliers highlighted
cooks_dt %>%
ggplot(aes(x = index, y = cooksdist)) +
geom_segment(aes(x = index, y = 0, xend = index, yend = cooksdist, color = Outlier)) +
scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
xlab("Index") +
ylab("Cook's Distance") +
theme_minimal() +
# Label the outlier points
geom_text(
data = filter(cooks_dt, is_outlier),
aes(label = index),
vjust = -1,
color = "red"
)
# Assuming `log_model$model` is your model, and you are computing Cook's distance
cooksdist <- cooks.distance(log_model$model)
# Create a data table with Cook's Distance and indices
cooks_dt <- data.table(index = 1:length(cooksdist), cooksdist = cooksdist)
# Determine the threshold for outliers
n <- length(cooksdist)  # Number of observations
threshold <- 50 * mean(cooks_dt$cooksdist, na.rm = TRUE)      # Common rule of thumb
# Add a column to flag outliers
cooks_dt <- cooks_dt %>%
mutate(Outlier = cooksdist > threshold)
# Plot with outliers highlighted
cooks_dt %>%
ggplot(aes(x = index, y = cooksdist)) +
geom_segment(aes(x = index, y = 0, xend = index, yend = cooksdist, color = Outlier)) +
scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
xlab("Index") +
ylab("Cook's Distance") +
ylim(0, 0.016) +
theme_minimal() +
# Label the outlier points
geom_text(
data = filter(cooks_dt, is_outlier),
aes(label = index),
vjust = -1,
color = "red"
)
# Plot with outliers highlighted
cooks_dt %>%
ggplot(aes(x = index, y = cooksdist)) +
geom_segment(aes(x = index, y = 0, xend = index, yend = cooksdist, color = Outlier)) +
scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
xlab("Index") +
ylab("Cook's Distance") +
ylim(0, 0.016) +
theme_minimal() +
# Label the outlier points
geom_text(
data = filter(cooks_dt, Outlier),
aes(label = index),
vjust = -1,
color = "red"
)
install.packages("xfun")
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(povmap, flextable, tidyverse, here, ggplot2, sf, sfnetworks,
kableExtra, data.table, viridis, paletteer, ggthemes,
gridExtra, grid, flextable, showtext, extrafont, bookdown,
officer, ggExtra, ggtext)
# here::here(load("data-clean/estimation_results/pmap_image.RData"))
load(here::here("data-clean", "estimation_results", "pmap_image.RData"))
shp_dt <- readRDS(here::here("data-clean", "estimation_results", "povshapefile.RDS"))
specify_decimal <- function(x, k) trimws(format(round(x, k), nsmall=k))
descriptives_dt <- readRDS("inst/postestimation/tables/descriptives.RDS")
# Table 1: Coefficient of Variation (CV) Table
cv_table <-
descriptives_dt$cv_table %>%
mutate(ebp_cv = specify_decimal(ebp_cv, 4),
direct_cv = specify_decimal(direct_cv, 4)) %>%
flextable() %>%
set_header_labels(indicator = "Area", ebp_cv = "EBP CV", direct_cv = "Direct CV") %>%
set_table_properties(width = 1, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
theme_box() %>%
set_caption("Comparison of Coefficient of Variation (CV) between EBP and Direct Estimates",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab1a"))
# Table 2: Basic Information
basicinfo_df <-
descriptives_dt$basicinfo_df %>%
flextable() %>%
set_header_labels(indicator = "Indicator", census = "Census", survey = "Survey") %>%
set_table_properties(width = 1, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
theme_box() %>%
set_caption("Basic Information on Units and Regions",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab1b"))
# Print the tables
cv_table
basicinfo_df
data.frame(`Indicator Class` = c("Household asset ownership status", "Education",
"Household-dwelling characteristics",
"Sanitation", "Energy", "Location controls"),
Variables = c("Dummies for household ownership of cars, internet, fridges, number of rooms,
computer phone, cellphone, landphone, sewing machine, washing machine,
stove, bicycle, motorcycle, radio",
"Household literacy rates, highest education attainment in household, whether ever
attended school",
"Household size, floor type, wall type, roof type, proportion of household members
in specific age category",
"Source of water, type of water pipe, garbage disposal type, toilet type",
"Type of fuel for cooking and lighting, heating source dummies",
"Provincial dummies, sector (rural, urban)")) %>%
flextable() %>%
set_table_properties(width = 1, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
autofit() %>%
set_caption(caption = "EBP Model (Regression Results)",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab1")) %>%
theme_box()
data.frame(Step = c("Survey & Census GMD Variable Harmonization",
"Equivalent feature distribution check",
"Model selection"),
`Process Explanation` = c("The same set of variables (deemed potential predictors of per capita
welfare) are created in the survey and census. We use the standard
Global Monitoring Database 2.0 Harmonization dictionary for the feature
creation process",
"We ensure that census and survey have similar distributions. In ideal
circumstances, the survey is drawn from the census i.e. a census and
survey in the same year. However, with the NHIES survey being from
2015 and the census in 2011, we perform a standard z-test to test that
for each feature the population mean (census 2011) is not
statistically different (p-value > 0.05) from the survey mean (NHIES
2015/16). We drop all variables for which we have to reject the (equal
means) null hypothesis.",
"Apply the LASSO algorithm with the ordinary least squares (OLS)
estimation. The use of the OLS model in this case may be less than
optimal being that the EB empirical model rather applies a linear
mixed effects approach.")) %>%
flextable() %>%
set_table_properties(width = 1, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
autofit() %>%
set_caption(caption = "Step-by-Step summary of variable selection process",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab2")) %>%
theme_box()
variables_dt <- read.csv("data-clean/estimation_results/variables.csv")
ebp_reportcoef_table(log_model, decimals = 3) %>%
merge(variables_dt, by.x = "Variable", by.y = "variables") %>%
dplyr::select(label, coeff, std_error) %>%
rename(Variable = label,
Coefficients = coeff,
`Standard Error` = std_error) %>%
flextable() %>%
set_table_properties(width = 1, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
autofit() %>%
set_caption(caption = "Mixed effects regression model",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab3")) %>%
theme_box()
ydump_dt <- fread("//esapov/esapov/NAM/GEO/Population/povmap/unitmodel_log.csv")
ydump_dt <-
ydump_dt %>%
mutate(hid = rep(census_dt$hid, 100)) %>%
group_by(hid, Domain) %>%
summarize(Simulated_Y = mean(Simulated_Y, na.rm = TRUE),
XBetahat = mean(XBetahat, na.rm = TRUE),
eta = mean(eta, na.rm = TRUE),
epsilon = mean(epsilon, na.rm = TRUE))
welfare_dt <- data.frame(Welfare = log(c(ydump_dt$Simulated_Y, survey_dt$wel_PPP)),
Source = factor(c(rep("Simulated Consumption",
length(ydump_dt$Simulated_Y)),
rep("Actual Consumption",
length(survey_dt$wel_PPP)))))
# Plot the overlaid histograms
ggplot(welfare_dt, aes(x = Welfare, fill = Source)) +
geom_histogram(aes(y = after_stat(density)), alpha = 0.7, position = "identity", bins = 40) +
geom_density(aes(color = Source)) +
scale_fill_viridis_d(alpha = 0.7, option = "D") +  # Use viridis palette for fill
scale_color_viridis_d(option = "D") +  # Use viridis palette for density lines
labs(x = "Log Welfare",
y = "Density") +
xlim(c(5, 15)) +
theme_minimal()
anova_dt <-
ebp_normalityfit(log_model) %>%
mutate(value = specify_decimal(value, 3)) %>%
t() %>%
as.data.frame()
colnames(anova_dt) <- anova_dt[1,]
anova_dt <- anova_dt[-1,]
anova_dt %>%
as.data.frame() %>%
flextable() %>%
set_header_labels(rsq_marginal = "marginal",
rsq_conditional = "conditional",
epsilon_skewness = "skewness",
epsilon_kurtosis = "kurtosis",
random_skewness = "skewness",
random_kurtosis = "kurtosis") %>%
add_header_row(values = c("Model R\u00B2", "(Error Term) \u03B5", "(Random Effect) \u03BC"),
colwidths = c(2, 2, 2)) %>%
set_table_properties(width = 0.5, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
theme_box() %>%
autofit() %>%
set_caption(caption = "Assessing Normality Assumptions",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab_normality"))
epsilon_values <- as.numeric(residuals(log_model$model, level = 0, type = "pearson"))
mu_values <- as.numeric(nlme::ranef(log_model$model)$"(Intercept)")
mu_values <- (mu_values - mean(mu_values, na.rm = TRUE)) / sd(mu_values, na.rm = TRUE)
xx <- ks.test(epsilon_values, "pnorm", mean = mean(epsilon_values), sd = sd(epsilon_values))
yy <- ks.test(mu_values, "pnorm", mean = mean(mu_values), sd = sd(mu_values))
plot_random <-
mu_values %>%
as.data.table() %>%
setnames(new = "values") %>%
ggplot() +
geom_density(aes(x = values), fill = "blue", alpha = 0.5) +
labs(x = "Random Effects", y = "Density", title = "Random Effect Residuals (\u03BC)") +
theme_minimal() +
ylim(0, 0.4) +
xlim(-4, 4) +
annotate("richtext", x = 0, y = 0.3, label = "<b>Kolmogrov-Smirnov Normality Test</b>",
fill = NA, label.color = NA) +
annotate("text", x = 0, y = 0.28, label = paste0("D-stat = ",
specify_decimal(yy$statistic, 3),
"; p-value = ",
specify_decimal(yy$p.value, 3)),
size = 3)
plot_error <-
epsilon_values %>%
as.data.table() %>%
setnames(new = "values") %>%
ggplot() +
geom_density(aes(x = values), fill = "blue", alpha = 0.5) +
labs(x = "Model Error Terms", y = "Density", title = "Standardized Error Residuals (\u03B5)") +
theme_minimal() +
annotate("richtext", x = 0.75, y = 0.3, label = "<b>Kolmogrov-Smirnov Normality Test</b>",
fill = NA, label.color = NA) +
annotate("text", x = 0, y = 0.28, label = paste0("D-stat = ",
specify_decimal(xx$statistic, 3),
"; p-value = ",
specify_decimal(xx$p.value, 3)),
size = 3)
grid.arrange(grobs = list(plot_random, plot_error), nrow = 1)
cv_dt <- read.csv("inst/postestimation/tables/targetarea_cvtable.csv")
gains_value <-
cv_dt %>%
mutate(gains = DesignEffect_CV / EBP_Head_Count_CV) %>%
summarize(mean(gains, na.rm = TRUE))
#### include a plot of design effect CV vs EBP head count CV
cv_scatter <-
cv_dt %>%
ggplot(aes(y = EBP_Head_Count_CV, x = DesignEffect_CV)) +
geom_point(color = "blue") +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
labs(x = "EBP Headcount CV",
y = "Design Effect Adjusted CV",
title = "Comparison of EBP Head Count CV and Design Effect Adjusted CV") +
xlim(0, 1.5) +
ylim(0, 1.5) +
annotate(
"text",
x = 0.75,  # Position of the text on the x-axis
y = 0.75,  # Position of the text on the y-axis
label = bquote("Precision Gains, " ~ frac(CV[EBP],
CV[DesignEffect]) ~ " = " ~ .(specify_decimal(gains_value,
2))),
fontface = "italic") +
theme_minimal()
plot_scatter <-
ggExtra::ggMarginal(cv_scatter, type = "histogram", fill = "lightblue", color = "black")
print(plot_scatter)
### include population from the census into the shapefile
hcrate_map <-
shp_dt %>%
ggplot() +
geom_sf(aes(fill = Head_Count), color = "white", size = 0.2) + # `geom_sf()` for spatial data
scale_fill_viridis(option = "A",  # Viridis color option
name = "Poverty Rate",
direction = -1) +
theme_minimal() # Minimal theme for a clean look
hccount_map <-
shp_dt %>%
ggplot() +
geom_sf(aes(fill = Head_Count*censuspop), color = "white", size = 0.2) +
scale_fill_viridis(option = "A",  # Viridis color option
name = "Population \n of Poor",
direction = -1) +
theme_minimal() # Minimal theme for a clean look
grid.arrange(grobs = list(hcrate_map, hccount_map),
nrow = 1)
pov_dt <-
log_model$ind %>%
merge(unique(census_dt[, c("region_code", "const_code")] %>%
mutate(const_code = as.factor(const_code))),
by.x = "Domain",
by.y = "const_code") %>%
merge(census_dt[,!duplicated(colnames(census_dt)), with = F] %>%
group_by(const_code) %>%
summarize(censuspop = sum(hhsize, na.rm = TRUE)) %>%
mutate(const_code = as.factor(const_code)),
by.x = "Domain",
by.y = "const_code")
newsurv_dt <- as.data.frame(na.omit(survey_dt[, c("wel_PPP",
nam_selvars_list,
"new_const_code",
"wta_hh",
"region_prev_name",
"region_prev_code",
"hhsize"),
with = F]))
newsurv_dt <- as.data.table(newsurv_dt)
regpov_dt <-
newsurv_dt[,!duplicated(colnames(newsurv_dt)), with = F] %>%
group_by(region_prev_name, region_prev_code) %>%
mutate(poor_var = ifelse(wel_PPP < 6249, 1, 0)) %>%
summarize(direct_poverty = weighted.mean(x = poor_var, w = wta_hh * hhsize, na.rm = TRUE)) %>%
merge(pov_dt %>%
group_by(region_code) %>%
summarize(ebp_poverty = weighted.mean(x = Head_Count,
w = censuspop,
na.rm = TRUE)),
by.x = "region_prev_code",
by.y = "region_code") %>%
mutate(direct_poverty = specify_decimal(direct_poverty, 3),
ebp_poverty = specify_decimal(ebp_poverty, 3)) %>%
select(region_prev_name, direct_poverty, ebp_poverty)
regpov_dt %>%
flextable() %>%
set_header_labels(region_prev_name = "Region",
direct_poverty = "Direct Survey Estimates",
ebp_poverty = "EBP Model Estimates") %>%
set_table_properties(width = 0.5, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
theme_box() %>%
autofit() %>%
set_caption(caption = "Regional Poverty Comparisons : Direct Survey vs Census EB estimates",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab_regpov"))
# Assuming `log_model$model` is your model, and you are computing Cook's distance
cooksdist <- cooks.distance(log_model$model)
# Create a data table with Cook's Distance and indices
cooks_dt <- data.table(index = 1:length(cooksdist), cooksdist = cooksdist)
# Determine the threshold for outliers
n <- length(cooksdist)  # Number of observations
threshold <- 50 * mean(cooks_dt$cooksdist, na.rm = TRUE)      # Common rule of thumb
# Add a column to flag outliers
cooks_dt <- cooks_dt %>%
mutate(Outlier = cooksdist > threshold)
# Plot with outliers highlighted
cooks_dt %>%
ggplot(aes(x = index, y = cooksdist)) +
geom_segment(aes(x = index, y = 0, xend = index, yend = cooksdist, color = Outlier)) +
scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
xlab("Index") +
ylab("Cook's Distance") +
ylim(0, 0.016) +
theme_minimal() +
# Label the outlier points
geom_text(
data = filter(cooks_dt, Outlier),
aes(label = index),
vjust = -1,
color = "red"
)
