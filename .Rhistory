theme_minimal() # Minimal theme for a clean look
grid.arrange(grobs = list(hcrate_map, hccount_map),
nrow = 1)
pov_dt <-
log_model$ind %>%
merge(unique(census_dt[, c("region_code", "const_code")] %>%
mutate(const_code = as.factor(const_code))),
by.x = "Domain",
by.y = "const_code") %>%
merge(census_dt[,!duplicated(colnames(census_dt)), with = F] %>%
group_by(const_code) %>%
summarize(censuspop = sum(hhsize, na.rm = TRUE)) %>%
mutate(const_code = as.factor(const_code)),
by.x = "Domain",
by.y = "const_code")
newsurv_dt <- as.data.frame(na.omit(survey_dt[, c("wel_PPP",
nam_selvars_list,
"new_const_code",
"wta_hh",
"region_prev_name",
"region_prev_code",
"hhsize"),
with = F]))
newsurv_dt <- as.data.table(newsurv_dt)
regpov_dt <-
newsurv_dt[,!duplicated(colnames(newsurv_dt)), with = F] %>%
group_by(region_prev_name, region_prev_code) %>%
mutate(poor_var = ifelse(wel_PPP < 6249, 1, 0)) %>%
summarize(direct_poverty = weighted.mean(x = poor_var, w = wta_hh * hhsize, na.rm = TRUE)) %>%
merge(pov_dt %>%
group_by(region_code) %>%
summarize(ebp_poverty = weighted.mean(x = Head_Count,
w = censuspop,
na.rm = TRUE)),
by.x = "region_prev_code",
by.y = "region_code") %>%
mutate(direct_poverty = specify_decimal(direct_poverty, 3),
ebp_poverty = specify_decimal(ebp_poverty, 3)) %>%
select(region_prev_name, direct_poverty, ebp_poverty)
regpov_dt %>%
flextable() %>%
set_header_labels(region_prev_name = "Region",
direct_poverty = "Direct Survey Estimates",
ebp_poverty = "EBP Model Estimates") %>%
set_table_properties(width = 0.5, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
theme_box() %>%
autofit() %>%
set_caption(caption = "Regional Poverty Comparisons : Direct Survey vs Census EB estimates",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab_regpov"))
# Assuming `log_model$model` is your model, and you are computing Cook's distance
cooksdist <- cooks.distance(log_model$model)
# Create a data table with Cook's Distance and indices
cooks_dt <- data.table(index = 1:length(cooksdist), cooksdist = cooksdist)
# Determine the threshold for outliers
n <- length(cooksdist)  # Number of observations
threshold <- 50 * mean(cooks_dt$cooksdist, na.rm = TRUE)      # Common rule of thumb
# Add a column to flag outliers
cooks_dt <- cooks_dt %>%
mutate(Outlier = cooksdist > threshold)
# Plot with outliers highlighted
cooks_dt %>%
ggplot(aes(x = index, y = cooksdist)) +
geom_segment(aes(x = index, y = 0, xend = index, yend = cooksdist, color = Outlier)) +
scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
xlab("Index") +
ylab("Cook's Distance") +
ylim(0, 0.016) +
theme_minimal() +
# Label the outlier points
geom_text(
data = filter(cooks_dt, Outlier),
aes(label = index),
vjust = -1,
color = "red"
)
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(povmap, flextable, tidyverse, here, ggplot2, sf, sfnetworks,
kableExtra, data.table, viridis, paletteer, ggthemes,
gridExtra, grid, flextable, showtext, extrafont, bookdown,
officer, ggExtra, ggtext)
# here::here(load("data-clean/estimation_results/pmap_image.RData"))
load(here::here("data-clean", "estimation_results", "pmap_image.RData"))
shp_dt <- readRDS(here::here("data-clean", "estimation_results", "povshapefile.RDS"))
specify_decimal <- function(x, k) trimws(format(round(x, k), nsmall=k))
descriptives_dt <- readRDS("inst/postestimation/tables/descriptives.RDS")
# Table 1: Coefficient of Variation (CV) Table
cv_table <-
descriptives_dt$cv_table %>%
mutate(ebp_cv = specify_decimal(ebp_cv, 4),
direct_cv = specify_decimal(direct_cv, 4)) %>%
flextable() %>%
set_header_labels(indicator = "Area", ebp_cv = "EBP CV", direct_cv = "Direct CV") %>%
set_table_properties(width = 1, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
theme_box() %>%
set_caption("Comparison of Coefficient of Variation (CV) between EBP and Direct Estimates",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab1a"))
# Table 2: Basic Information
basicinfo_df <-
descriptives_dt$basicinfo_df %>%
flextable() %>%
set_header_labels(indicator = "Indicator", census = "Census", survey = "Survey") %>%
set_table_properties(width = 1, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
theme_box() %>%
set_caption("Basic Information on Units and Regions",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab1b"))
# Print the tables
cv_table
basicinfo_df
data.frame(`Indicator Class` = c("Household asset ownership status", "Education",
"Household-dwelling characteristics",
"Sanitation", "Energy", "Location controls"),
Variables = c("Dummies for household ownership of cars, internet, fridges, number of rooms,
computer phone, cellphone, landphone, sewing machine, washing machine,
stove, bicycle, motorcycle, radio",
"Household literacy rates, highest education attainment in household, whether ever
attended school",
"Household size, floor type, wall type, roof type, proportion of household members
in specific age category",
"Source of water, type of water pipe, garbage disposal type, toilet type",
"Type of fuel for cooking and lighting, heating source dummies",
"Provincial dummies, sector (rural, urban)")) %>%
flextable() %>%
set_table_properties(width = 1, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
autofit() %>%
set_caption(caption = "EBP Model (Regression Results)",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab1")) %>%
theme_box()
data.frame(Step = c("Survey & Census GMD Variable Harmonization",
"Equivalent feature distribution check",
"Model selection"),
`Process Explanation` = c("The same set of variables (deemed potential predictors of per capita
welfare) are created in the survey and census. We use the standard
Global Monitoring Database 2.0 Harmonization dictionary for the feature
creation process",
"We ensure that census and survey have similar distributions. In ideal
circumstances, the survey is drawn from the census i.e. a census and
survey in the same year. However, with the NHIES survey being from
2015 and the census in 2011, we perform a standard z-test to test that
for each feature the population mean (census 2011) is not
statistically different (p-value > 0.05) from the survey mean (NHIES
2015/16). We drop all variables for which we have to reject the (equal
means) null hypothesis.",
"Apply the LASSO algorithm with the ordinary least squares (OLS)
estimation. The use of the OLS model in this case may be less than
optimal being that the EB empirical model rather applies a linear
mixed effects approach.")) %>%
flextable() %>%
set_table_properties(width = 1, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
autofit() %>%
set_caption(caption = "Step-by-Step summary of variable selection process",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab2")) %>%
theme_box()
variables_dt <- read.csv("data-clean/estimation_results/variables.csv")
ebp_reportcoef_table(log_model, decimals = 3) %>%
merge(variables_dt, by.x = "Variable", by.y = "variables") %>%
dplyr::select(label, coeff, std_error) %>%
rename(Variable = label,
Coefficients = coeff,
`Standard Error` = std_error) %>%
flextable() %>%
set_table_properties(width = 1, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
autofit() %>%
set_caption(caption = "Mixed effects regression model",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab3")) %>%
theme_box()
variables_dt
variables_dt <- read.csv("data-clean/estimation_results/variables.csv")
ebp_reportcoef_table(log_model, decimals = 3) %>%
merge(variables_dt, by.x = "Variable", by.y = "variables") %>%
dplyr::select(label, coeff, std_error) %>%
rename(Variable = label,
Coefficients = coeff,
`Standard Error` = std_error) %>%
flextable() %>%
set_table_properties(width = 1, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
autofit() %>%
set_caption(caption = "Mixed effects regression model",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab3")) %>%
theme_box()
anova_dt <-
ebp_normalityfit(log_model) %>%
mutate(value = specify_decimal(value, 3)) %>%
t() %>%
as.data.frame()
colnames(anova_dt) <- anova_dt[1,]
anova_dt <- anova_dt[-1,]
anova_dt %>%
as.data.frame() %>%
flextable() %>%
set_header_labels(rsq_marginal = "marginal",
rsq_conditional = "conditional",
epsilon_skewness = "skewness",
epsilon_kurtosis = "kurtosis",
random_skewness = "skewness",
random_kurtosis = "kurtosis") %>%
add_header_row(values = c("Model R\u00B2", "(Error Term) \u03B5", "(Random Effect) \u03BC"),
colwidths = c(2, 2, 2)) %>%
set_table_properties(width = 0.5, layout = "autofit") %>%
fontsize(size = 10) %>%
font(fontname = "Times New Roman", part = "all") %>%
theme_box() %>%
autofit() %>%
set_caption(caption = "Assessing Normality Assumptions",
style = "Table Caption",
autonum = run_autonum(seq_id = "tab", bkm = "tab_normality"))
anova_dt$random_skewness
anova_dt
specify_decimal(anova_dt$random_skewness, 3)
specify_decimal(as.numeric(anova_dt$random_kurtosis), 3)
ydump_dt <- fread("//esapov/esapov/NAM/GEO/Population/povmap/unitmodel_log.csv")
ydump_dt <-
ydump_dt %>%
mutate(hid = rep(census_dt$hid, 100)) %>%
group_by(hid, Domain) %>%
summarize(Simulated_Y = mean(Simulated_Y, na.rm = TRUE),
XBetahat = mean(XBetahat, na.rm = TRUE),
eta = mean(eta, na.rm = TRUE),
epsilon = mean(epsilon, na.rm = TRUE))
saveRDS(ydump_dt, "data-clean/estimation_results/welfareydump_dt")
epsilon_values <- as.numeric(residuals(log_model$model, level = 0, type = "pearson"))
mu_values <- as.numeric(nlme::ranef(log_model$model)$"(Intercept)")
mu_values <- (mu_values - mean(mu_values, na.rm = TRUE)) / sd(mu_values, na.rm = TRUE)
xx <- ks.test(epsilon_values, "pnorm", mean = mean(epsilon_values), sd = sd(epsilon_values))
yy <- ks.test(mu_values, "pnorm", mean = mean(mu_values), sd = sd(mu_values))
plot_random <-
mu_values %>%
as.data.table() %>%
setnames(new = "values") %>%
ggplot() +
geom_density(aes(x = values), fill = "blue", alpha = 0.5) +
labs(x = "Random Effects", y = "Density", title = "Random Effect Residuals (\u03BC)") +
theme_minimal() +
ylim(0, 0.4) +
xlim(-4, 4) +
annotate("richtext", x = 0, y = 0.3, label = "<b>Kolmogrov-Smirnov Normality Test</b>",
fill = NA, label.color = NA) +
annotate("text", x = 0, y = 0.28, label = paste0("D-stat = ",
specify_decimal(yy$statistic, 3),
"; p-value = ",
specify_decimal(yy$p.value, 3)),
size = 3)
plot_error <-
epsilon_values %>%
as.data.table() %>%
setnames(new = "values") %>%
ggplot() +
geom_density(aes(x = values), fill = "blue", alpha = 0.5) +
labs(x = "Model Error Terms", y = "Density", title = "Standardized Error Residuals (\u03B5)") +
theme_minimal() +
annotate("richtext", x = 0.75, y = 0.3, label = "<b>Kolmogrov-Smirnov Normality Test</b>",
fill = NA, label.color = NA) +
annotate("text", x = 0, y = 0.28, label = paste0("D-stat = ",
specify_decimal(xx$statistic, 3),
"; p-value = ",
specify_decimal(xx$p.value, 3)),
size = 3)
grid.arrange(grobs = list(plot_random, plot_error), nrow = 1)
cv_dt <- read.csv("inst/postestimation/tables/targetarea_cvtable.csv")
gains_value <-
cv_dt %>%
mutate(gains = DesignEffect_CV / EBP_Head_Count_CV) %>%
summarize(mean(gains, na.rm = TRUE))
#### include a plot of design effect CV vs EBP head count CV
cv_scatter <-
cv_dt %>%
ggplot(aes(y = EBP_Head_Count_CV, x = DesignEffect_CV)) +
geom_point(color = "blue") +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +
labs(x = "EBP Headcount CV",
y = "Design Effect Adjusted CV",
title = "Comparison of EBP Head Count CV and Design Effect Adjusted CV") +
xlim(0, 1.5) +
ylim(0, 1.5) +
annotate(
"text",
x = 0.75,  # Position of the text on the x-axis
y = 0.75,  # Position of the text on the y-axis
label = bquote("Precision Gains, " ~ frac(CV[EBP],
CV[DesignEffect]) ~ " = " ~ .(specify_decimal(gains_value,
2))),
fontface = "italic") +
theme_minimal()
plot_scatter <-
ggExtra::ggMarginal(cv_scatter, type = "histogram", fill = "lightblue", color = "black")
print(plot_scatter)
survey_dt[, c("region_code", "region_name")]
unique(survey_dt[, c("region_code", "region_name")])
unique(survey_dt[, c("region_prev_code", "region_prev_name")])
knitr::opts_chunk$set(echo = TRUE)
pacman::p_load(povmap, flextable, tidyverse, here, ggplot2, sf, sfnetworks,
kableExtra, data.table, viridis, paletteer, ggthemes,
gridExtra, grid, flextable, showtext, extrafont, bookdown,
officer, ggExtra, ggtext)
# here::here(load("data-clean/estimation_results/pmap_image.RData"))
load(here::here("data-clean", "estimation_results", "pmap_image.RData"))
shp_dt <- readRDS(here::here("data-clean", "estimation_results", "povshapefile.RDS"))
unique(census_dt$region_code)
unique(census_dt$region_name)
unique(survey_dt$region_code)
shp_dt
length(unique(survey_dt$const_code))
unique(survey_dt$const_code)
unique(census_dt$const_code)
length(unique(census_dt$const_code))
#------------------------------------------------------------------------------#
## environment set up
#------------------------------------------------------------------------------#
remove(list = objects())
options(
stringsAsFactors = F, ## tell R to treat text as text, not factors
width = 100, ## set the maximum width of outputs as 80 characters
scipen = 6, ## discourage R from displaying numbers in scientific notation
start.time= Sys.time()
)
#------------------------------------------------------------------------------#
pacman::p_load(dplyr, here, descr, data.table, tidyr, stats,  haven)
# source(here("R/01_littleworkerfunctions.R"))
load(here("inst/data/census_dt.RData"))
load(here("inst/data/survey_dt.RData"))
pacman::p_load(tidyverse, data.table)
census_dt <- haven::read_dta("data-raw/Census 2011/Housing_2011.dta")
colnames(census_dt)
## Meta-Information ######------######------######------
## Author: Hardika Dayalani (hardika.dayalani@gmail.com)
## Creation: Namibia Small Area Poverty Estimates
## Description: Clean Census 2011 data for harmonization
## Environment Setup ######------######------######------
rm(list =  ls())
pacman::p_load(here, haven, data.table)
## Set Working Directory
# getwd()
# setwd("C:\Users\wb604749\OneDrive - WBG\Documents\GitHub\NamibiaPovMap\data-raw\Namibia_Data\Census 2011")
## Load Libraries
## Geographic Data ######------######------######------
## Import Constituency Data
const_code_df <-  read.csv("Namibia_Constituency_Code.txt", header = FALSE)
const_code_df <-  read.csv("inst/scripts/CensusHarmonizationFiles/Namibia_Constituency_Code.txt", header = FALSE)
const_code
const_code_df
## Clean Constituency Data
names(const_code_df) <-  c("const_code", "region", "constituency")
const_code_df$const_code <-  sprintf("%04.f", const_code_df$const_code)
const_code_df$region_code <-  substr(const_code_df$const_code, 1, 2)
## Region Code .
region_code_df <-  const_code_df[, c("region", "region_code")]
region_code_df <-  unique(region_code_df)
house_df <-  read_dta("data-raw/Census2011/Housing_2011.dta")
names(house_df) <-  tolower(names(house_df))
View(house_df)
## Subset relevant variables
temp <-  c("region", "constituency", "hh_type", "ea_code", "du_number",
"hh_number", "h1", "h4", "h5", "h6", "h7", "h8a",
"h8b", "h8c", "h9", "h10", "h14")
house_df <-  house_df[, temp]
## Dropped 11 variables from 28 to 17
## Subset to conventional households
##house_df <-  house_df[house_df$hh_type == 100, ]
## region names
house_df$region_code <-  sprintf("%02.f", house_df$region)
temp <-  match(house_df$region_code, region_code_df$region_code)
house_df$region_name <-  region_code_df$region[temp]
## constituency
house_df$const_code <-  paste0(house_df$region_code, sprintf("%02.f", house_df$constituency))
temp <-  match(house_df$const_code, const_code_df$const_code)
house_df$constituency_name <-  const_code_df$constituency[temp]
str(house_df[!is.na(house_df$hh_number),])
View(house_df)
View(house_df)
house_df <-  read_dta("data-raw/Census2011/Housing_2011.dta")
View(house_df)
glimpse(house_df)
dplyr::glimpse(house_df)
summary(house_df$H12K)
summary(house_df$H12L)
summary(house_df$H12M)
summary(house_df$H5)
View(house_df)
glimpse(house_df[!is.na(house_df$HH_NUMBER),])
dplyr::glimpse(house_df[!is.na(house_df$HH_NUMBER),])
## Meta-Information ######------######------######------
## Author: Hardika Dayalani (hardika.dayalani@gmail.com)
## Creation: Namibia Small Area Poverty Estimates
## Description: Clean Census 2011 data for harmonization
## Environment Setup ######------######------######------
rm(list =  ls())
pacman::p_load(here, haven, data.table)
## Set Working Directory
# getwd()
# setwd("C:\Users\wb604749\OneDrive - WBG\Documents\GitHub\NamibiaPovMap\data-raw\Namibia_Data\Census 2011")
## Load Libraries
## Geographic Data ######------######------######------
## Import Constituency Data
const_code_df <-  read.csv("inst/scripts/CensusHarmonizationFiles/Namibia_Constituency_Code.txt", header = FALSE)
## Clean Constituency Data
names(const_code_df) <-  c("const_code", "region", "constituency")
const_code_df$const_code <-  sprintf("%04.f", const_code_df$const_code)
const_code_df$region_code <-  substr(const_code_df$const_code, 1, 2)
## Region Code .
region_code_df <-  const_code_df[, c("region", "region_code")]
region_code_df <-  unique(region_code_df)
## Housing Data ######------######------######------
## Import Data
house_df <-  read_dta("data-raw/Census2011/Housing_2011.dta")
names(house_df) <-  tolower(names(house_df))
per_df <-  read_dta("data-raw/Census2011/Persons_2011.dta")
View(per_df)
summary(per_df$HH_NUMBER)
View(const_code_df)
survey_dt <- readRDS("data-clean/survey_dt.RDS")
colnames(survey_dt)
#----------------------- EBP MODEL  ESTAIMATION -------------------------------#
## environment set up
remove(list = objects())
options(
stringsAsFactors = F,
width = 100,
scipen = 6,
start.time= Sys.time()
)
pacman::p_load(data.table, dplyr, stringr, fuzzyjoin, povmap)
#------------------------------------------------------------------------------#
nam_selvars_list <- readRDS("data-raw/nam_selvars_list.RDS")
census_dt <- readRDS("data-clean/census_dt.RDS")
survey_dt <- readRDS("data-clean/survey_dt.RDS")
census_dt$const_code <- as.numeric(census_dt$const_code)
survey_dt$new_const_code <- as.numeric(survey_dt$new_const_code)
unique(survey_dt[, c("region", "new_const_code")])
unique(survey_dt[, c("region_name", "new_const_code")])
View(unique(survey_dt[, c("region_name", "new_const_code")]))
colnames(survey_dt)
View(unique(survey_dt[, c("region_name", "region_code", "new_const_code", "constituency_name", "const_code")]))
census_dt["wta_hh"] <- lapply(haven::zap_labels(census_dt["wta_hh"]) ,as.numeric)
nam_selvars_list  <- intersect(intersect(nam_selvars_list, names(survey_dt)), names(census_dt))
census_dt <- as.data.table(census_dt)
survey_dt <- as.data.table(survey_dt)
survey_dt[, !duplicated(colnames(survey_dt)), with = F] %>%
group_by(region_name, region_code) %>%
mutate(poor_var = ifelse(wel_PPP < 6249, 1, 0)) %>%
summarize(weighted.mean(x = poor_var,
w = wta_hh * hhsize,
na.rm = TRUE))
unique(survey_dt$pl_fd)
unique(survey_dt$pl_abs)
survey_dt[, !duplicated(colnames(survey_dt)), with = F] %>%
group_by(region_name, region_code) %>%
mutate(poor_var = ifelse(wel_PPP < 6249.437, 1, 0)) %>%
summarize(weighted.mean(x = poor_var,
w = wta_hh,
na.rm = TRUE))
survey_dt[, !duplicated(colnames(survey_dt)), with = F] %>%
group_by(region_name, region_code) %>%
mutate(poor_var = ifelse(wel_PPP < 6249.437, 1, 0)) %>%
summarize(weighted.mean(x = poor_var,
w = wta_pop,
na.rm = TRUE))
survey_dt[, !duplicated(colnames(survey_dt)), with = F] %>%
group_by(region_name, region_code) %>%
# mutate(poor_var = ifelse(wel_PPP < 6249.437, 1, 0)) %>%
summarize(weighted.mean(x = poor_abs,
w = wta_pop,
na.rm = TRUE))
summary(survey_dt$poor_abs)
survey_dt$poor_abs
survey_dt[, !duplicated(colnames(survey_dt)), with = F] %>%
mutate(poor_abs = ifelse(poor_abs == "Poor", 1, 0)) %>%
group_by(region_name, region_code) %>%
# mutate(poor_var = ifelse(wel_PPP < 6249.437, 1, 0)) %>%
summarize(weighted.mean(x = poor_abs,
w = wta_pop,
na.rm = TRUE))
survey_dt[, !duplicated(colnames(survey_dt)), with = F] %>%
mutate(poor_abs = ifelse(poor_abs == "Poor", 1, 0)) %>%
group_by(prev_region_name, region_code) %>%
# mutate(poor_var = ifelse(wel_PPP < 6249.437, 1, 0)) %>%
summarize(weighted.mean(x = poor_abs,
w = wta_pop,
na.rm = TRUE))
survey_dt[, !duplicated(colnames(survey_dt)), with = F] %>%
mutate(poor_abs = ifelse(poor_abs == "Poor", 1, 0)) %>%
group_by(region_prev_name, region_code) %>%
# mutate(poor_var = ifelse(wel_PPP < 6249.437, 1, 0)) %>%
summarize(weighted.mean(x = poor_abs,
w = wta_pop,
na.rm = TRUE))
survey_dt[, !duplicated(colnames(survey_dt)), with = F] %>%
mutate(poor_abs = ifelse(poor_abs == "Poor", 1, 0)) %>%
group_by(region_code) %>%
# mutate(poor_var = ifelse(wel_PPP < 6249.437, 1, 0)) %>%
summarize(weighted.mean(x = poor_abs,
w = wta_pop,
na.rm = TRUE))
